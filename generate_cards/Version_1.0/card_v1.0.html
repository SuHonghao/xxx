
<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <title>card</title>
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <section class="card-list">
    
    <article class="cite-card">
      <a class="thumb" href="https://www.sciencedirect.com/science/article/pii/S0278612525001657" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/Multi-robot collaborative manufacturing driven by digital twins_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://www.sciencedirect.com/science/article/pii/S0278612525001657" target="_blank" rel="noopener">Multi-robot collaborative manufacturing driven by digital twins: Advancements, challenges, and future directions</a>
        </h4>
        <p class="cite-brief">Multi-robot systems envisioned for future factories will promote advancements and capabilities of handling complex tasks and realising optimal robotic operations. However, existing multi-robot systems face challenges such as integration complexity, difficult coordination and control, low scalability, and flexibility, and thus are far from realising adaptive and efficient multi-robot collaborative manufacturing (MRCM). Digital twin technology improves visualisation, consistency, and spatial–temporal collaboration in MRCM through real-time interaction and iterative optimisation in physical and virtual spaces. Despite these improvements, barriers such as undeveloped modelling capabilities, indeterminate collaborative strategies, and limited applicability impede widespread integration of MRCM. In response to these needs, this study provides a comprehensive review of the foundational concepts, systematic architecture, and enabling technologies of digital twin-driven MRCM, serving as a prospective vision for future work in collaborative intelligent manufacturing. With the development of sensors and computational capabilities, robot intelligence is evolving towards multi-robot collaboration, including perceptual, cognitive, and behavioural collaboration. Digital twins play a critical supporting role in multi-robot collaboration, and the architecture, methodologies, and applications are elaborated across diverse stages of MRCM processes. This paper also identifies current challenges and future research directions. It encourages academic and industrial stakeholders to integrate state-of-the-art AI technologies more thoroughly into multi-robot digital twin systems for enhanced efficiency and reliability in production.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Multi-Robot Collaborative Manufacturing</span><span class="tag">Digital Twin</span><span class="tag">Intelligent Manufacturing</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2503.03081" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/AirExo-2_img.png" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2503.03081" target="_blank" rel="noopener">AirExo-2: Scaling up Generalizable Robotic Imitation Learning with Low-Cost Exoskeletons</a>
        </h4>
        <p class="cite-brief">Scaling up robotic imitation learning for real-world applications requires efficient and scalable demonstration collection methods. While teleoperation is effective, it depends on costly and inflexible robot platforms. In-the-wild demonstrations offer a promising alternative, but existing collection devices have key limitations: handheld setups offer limited observational coverage, and whole-body systems often require fine-tuning with robot data due to domain gaps. To address these challenges, we present AirExo-2, a low-cost exoskeleton system for large-scale in-the-wild data collection, along with several adaptors that transform collected data into pseudo-robot demonstrations suitable for policy learning. We further introduce RISE-2, a generalizable imitation learning policy that fuses 3D spatial and 2D semantic perception for robust manipulations. Experiments show that RISE-2 outperforms prior state-of-the-art methods on both in-domain and generalization evaluations. Trained solely on adapted in-the-wild data produced by AirExo-2, the RISE-2 policy achieves comparable performance to the policy trained with teleoperated data, highlighting the effectiveness and potential of AirExo-2 for scalable and generalizable imitation learning.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">AirExo-2</span><span class="tag">In-the-Wild Data Collection</span><span class="tag">Imitation Learning</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2506.18088" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/RoboTwin 2.0_img.png" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2506.18088" target="_blank" rel="noopener">RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation</a>
        </h4>
        <p class="cite-brief">Simulation-based data synthesis has emerged as a powerful paradigm for advancing real-world robotic manipulation. Yet existing datasets remain insufficient for robust bimanual manipulation due to (1) the lack of scalable task generation methods and (2) oversimplified simulation environments. We present RoboTwin 2.0, a scalable framework for automated, large-scale generation of diverse and realistic data, together with unified evaluation protocols for dual-arm manipulation. At its core is RoboTwin-OD, an object library of 731 instances across 147 categories with semantic and manipulation-relevant annotations. Building on this, we design an expert data synthesis pipeline that leverages multimodal language models (MLLMs) and simulation-in-the-loop refinement to automatically generate task-level execution code. To improve sim-to-real transfer, RoboTwin 2.0 applies structured domain randomization along five axes: clutter, lighting, background, tabletop height, and language, enhancing data diversity and policy robustness. The framework is instantiated across 50 dual-arm tasks and five robot embodiments. Empirically, it yields a 10.9% gain in code generation success rate. For downstream policy learning, a VLA model trained with synthetic data plus only 10 real demonstrations achieves a 367% relative improvement over the 10-demo baseline, while zero-shot models trained solely on synthetic data obtain a 228% gain. These results highlight the effectiveness of RoboTwin 2.0 in strengthening sim-to-real transfer and robustness to environmental variations. We release the data generator, benchmark, dataset, and code to support scalable research in robust bimanual manipulation. </p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">RoboTwin 2.0</span><span class="tag">Bimanual Manipulation</span><span class="tag">Simulation-Based Data Synthesis</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2505.14030" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/AutoBio_img.png" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2505.14030" target="_blank" rel="noopener">AutoBio: A Simulation and Benchmark for Robotic Automation in Digital Biology Laboratory</a>
        </h4>
        <p class="cite-brief">Vision-language-action (VLA) models have shown promise as generalist robotic policies by jointly leveraging visual, linguistic, and proprioceptive modalities to generate action trajectories. While recent benchmarks have advanced VLA research in domestic tasks, professional science-oriented domains remain underexplored. We introduce AutoBio, a simulation framework and benchmark designed to evaluate robotic automation in biology laboratory environments--an application domain that combines structured protocols with demanding precision and multimodal interaction. AutoBio extends existing simulation capabilities through a pipeline for digitizing real-world laboratory instruments, specialized physics plugins for mechanisms ubiquitous in laboratory workflows, and a rendering stack that support dynamic instrument interfaces and transparent materials through physically based rendering. Our benchmark comprises biologically grounded tasks spanning three difficulty levels, enabling standardized evaluation of language-guided robotic manipulation in experimental protocols. We provide infrastructure for demonstration generation and seamless integration with VLA models. Baseline evaluations with two SOTA VLA models reveal significant gaps in precision manipulation, visual reasoning, and instruction following in scientific workflows. By releasing AutoBio, we aim to catalyze research on generalist robotic systems for complex, high-precision, and multimodal professional environments. The simulator and benchmark are publicly available to facilitate reproducible research.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">AutoBio</span><span class="tag">Vision-Language-Action Models</span><span class="tag">Biology Laboratory Automation</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.175502755.53627529" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/Vision-Language-Action Models for Robotics_img.png" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.175502755.53627529" target="_blank" rel="noopener">Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</a>
        </h4>
        <p class="cite-brief">Amid growing efforts to leverage advances in large language models (LLMs) and vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models have recently gained significant attention. By unifying vision, language, and action data at scale, which have traditionally been studied separately, VLA models aim to learn policies that generalise across diverse tasks, objects, embodiments, and environments. This generalisation capability is expected to enable robots to solve novel downstream tasks with minimal or no additional task-specific data, facilitating more flexible and scalable realworld deployment. Unlike previous surveys that focus narrowly on action representations or high-level model architectures, this work offers a comprehensive, full-stack review, integrating both software and hardware components of VLA systems. In particular, this paper provides a systematic review of VLAs, covering their strategy and architectural transition, architectures and building blocks, modality-specific processing techniques, and learning paradigms. In addition, to support the deployment of VLAs in real-world robotic applications, we also review commonly used robot platforms, data collection strategies, publicly available datasets, data augmentation methods, and evaluation benchmarks. Throughout this comprehensive survey, this paper aims to offer practical guidance for the robotics community in applying VLAs to real-world robotic systems.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Vision-Language-Action Models</span><span class="tag">Robotics Survey</span><span class="tag">Generalization in Embodied AI</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2505.23171" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/RoboTransfer_img.png" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2505.23171" target="_blank" rel="noopener">RoboTransfer: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer</a>
        </h4>
        <p class="cite-brief">Imitation Learning has become a fundamental approach in robotic manipulation. However, collecting large-scale real-world robot demonstrations is prohibitively expensive. Simulators offer a cost-effective alternative, but the sim-to-real gap make it extremely challenging to scale. Therefore, we introduce RoboTransfer, a diffusion-based video generation framework for robotic data synthesis. Unlike previous methods, RoboTransfer integrates multi-view geometry with explicit control over scene components, such as background and object attributes. By incorporating cross-view feature interactions and global depth/normal conditions, RoboTransfer ensures geometry consistency across views. This framework allows fine-grained control, including background edits and object swaps. Experiments demonstrate that RoboTransfer is capable of generating multi-view videos with enhanced geometric consistency and visual fidelity. In addition, policies trained on the data generated by RoboTransfer achieve a 33.3% relative improvement in the success rate in the DIFF-OBJ setting and a substantial 251% relative improvement in the more challenging DIFF-ALL scenario. </p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">RoboTransfer</span><span class="tag">Diffusion-Based Video Generation</span><span class="tag">Sim-to-Real Transfer</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2507.05198" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/EmbodieDreamer_img.png" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2507.05198" target="_blank" rel="noopener">EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling</a>
        </h4>
        <p class="cite-brief">The rapid advancement of Embodied AI has led to an increasing demand for large-scale, high-quality real-world data. However, collecting such embodied data remains costly and inefficient. As a result, simulation environments have become a crucial surrogate for training robot policies. Yet, the significant Real2Sim2Real gap remains a critical bottleneck, particularly in terms of physical dynamics and visual appearance. To address this challenge, we propose EmbodieDreamer, a novel framework that reduces the Real2Sim2Real gap from both the physics and appearance perspectives. Specifically, we propose PhysAligner, a differentiable physics module designed to reduce the Real2Sim physical gap. It jointly optimizes robot-specific parameters such as control gains and friction coefficients to better align simulated dynamics with real-world observations. In addition, we introduce VisAligner, which incorporates a conditional video diffusion model to bridge the Sim2Real appearance gap by translating low-fidelity simulated renderings into photorealistic videos conditioned on simulation states, enabling high-fidelity visual transfer. Extensive experiments validate the effectiveness of EmbodieDreamer. The proposed PhysAligner reduces physical parameter estimation error by 3.74% compared to simulated annealing methods while improving optimization speed by 89.91/%. Moreover, training robot policies in the generated photorealistic environment leads to a 29.17% improvement in the average task success rate across real-world tasks after reinforcement learning. Code, model and data will be publicly available.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">EmbodieDreamer</span><span class="tag">Real2Sim2Real Gap</span><span class="tag">Differentiable Physics and Visual Alignment</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2505.19017" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/WorldEval_img.png" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2505.19017" target="_blank" rel="noopener">WorldEval: World Model as Real-World Robot Policies Evaluator</a>
        </h4>
        <p class="cite-brief">The field of robotics has made significant strides toward developing generalist robot manipulation policies. However, evaluating these policies in real-world scenarios remains time-consuming and challenging, particularly as the number of tasks scales and environmental conditions change. In this work, we demonstrate that world models can serve as a scalable, reproducible, and reliable proxy for real-world robot policy evaluation. A key challenge is generating accurate policy videos from world models that faithfully reflect the robot actions. We observe that directly inputting robot actions or using high-dimensional encoding methods often fails to generate action-following videos. To address this, we propose Policy2Vec, a simple yet effective approach to turn a video generation model into a world simulator that follows latent action to generate the robot video. We then introduce WorldEval, an automated pipeline designed to evaluate real-world robot policies entirely online. WorldEval effectively ranks various robot policies and individual checkpoints within a single policy, and functions as a safety detector to prevent dangerous actions by newly developed robot models. Through comprehensive paired evaluations of manipulation policies in real-world environments, we demonstrate a strong correlation between policy performance in WorldEval and real-world scenarios. Furthermore, our method significantly outperforms popular methods such as real-to-sim approach.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Policy2Vec</span><span class="tag">WorldEval</span><span class="tag">Robot Policy Evaluation</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2508.05342" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/Information-Theoretic Graph Fusion.png" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2508.05342" target="_blank" rel="noopener">Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control</a>
        </h4>
        <p class="cite-brief">Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB and Depth human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95 percent graph accuracy and 93 percent subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94 percent grasp success, 89 percent placement accuracy, and 90 percent overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">GF-VLA</span><span class="tag">Graph-Based Scene Representation</span><span class="tag">Dual-Arm Robotic Manipulation</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2506.23351" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/Benchmarking Generalizable Bimanual Manipulation_img.png" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2506.23351" target="_blank" rel="noopener">Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop</a>
        </h4>
        <p class="cite-brief">Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in robotics, driven by the need for autonomous systems that can perceive, reason, and act in complex physical environments. While single-arm systems have shown strong task performance, collaborative dual-arm systems are essential for handling more intricate tasks involving rigid, deformable, and tactile-sensitive objects. To advance this goal, we launched the RoboTwin Dual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on the RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot platform, the competition consisted of three stages: Simulation Round 1, Simulation Round 2, and a final Real-World Round. Participants totally tackled 17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based scenarios. The challenge attracted 64 global teams and over 400 participants, producing top-performing solutions like SEM and AnchorDP3 and generating valuable insights into generalizable bimanual policy learning. This report outlines the competition setup, task design, evaluation methodology, key findings and future direction, aiming to support future research on robust and generalizable bimanual manipulation policies. </p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">RoboTwin Challenge</span><span class="tag">Dual-Arm Collaboration</span><span class="tag">Bimanual Manipulation</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2508.17600" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/GWM_img.png" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2508.17600" target="_blank" rel="noopener">GWM: Towards Scalable Gaussian World Models for Robotic Manipulation</a>
        </h4>
        <p class="cite-brief">Training robot policies within a learned world model is trending due to the inefficiency of real-world interactions. The established image-based world models and policies have shown prior success, but lack robust geometric information that requires consistent spatial and physical understanding of the three-dimensional world, even pre-trained on internet-scale video sources. To this end, we propose a novel branch of world model named Gaussian World Model (GWM) for robotic manipulation, which reconstructs the future state by inferring the propagation of Gaussian primitives under the effect of robot actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D variational autoencoder, enabling fine-grained scene-level future state reconstruction with Gaussian Splatting. GWM can not only enhance the visual representation for imitation learning agent by self-supervised future prediction training, but can serve as a neural simulator that supports model-based reinforcement learning. Both simulated and real-world experiments depict that GWM can precisely predict future scenes conditioned on diverse robot actions, and can be further utilized to train policies that outperform the state-of-the-art by impressive margins, showcasing the initial data scaling potential of 3D world model.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Gaussian World Model</span><span class="tag">Diffusion Transformer</span><span class="tag">Robotic Manipulation</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2508.02629" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/HyCodePolicy_img.png" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2508.02629" target="_blank" rel="noopener">HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents</a>
        </h4>
        <p class="cite-brief">Recent advances in multimodal large language models (MLLMs) have enabled richer perceptual grounding for code policy generation in embodied agents. However, most existing systems lack effective mechanisms to adaptively monitor policy execution and repair codes during task completion. In this work, we introduce HyCodePolicy, a hybrid language-based control framework that systematically integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair into a closed-loop programming cycle for embodied agents. Technically, given a natural language instruction, our system first decomposes it into subgoals and generates an initial executable program grounded in object-centric geometric primitives. The program is then executed in simulation, while a vision-language model (VLM) observes selected checkpoints to detect and localize execution failures and infer failure reasons. By fusing structured execution traces capturing program-level events with VLM-based perceptual feedback, HyCodePolicy infers failure causes and repairs programs. This hybrid dual feedback mechanism enables self-correcting program synthesis with minimal human supervision. Our results demonstrate that HyCodePolicy significantly improves the robustness and sample efficiency of robot manipulation policies, offering a scalable strategy for integrating multimodal reasoning into autonomous decision-making pipelines.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">HyCodePolicy</span><span class="tag">Code Synthesis and Repair</span><span class="tag">Embodied Agents</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://www.mdpi.com/2673-4052/6/3/41" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/Design and Development_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://www.mdpi.com/2673-4052/6/3/41" target="_blank" rel="noopener">Design and Development of Cost-Effective Humanoid Robots for Enhanced Human–Robot Interaction</a>
        </h4>
        <p class="cite-brief">Industry Revolution Five (Industry 5.0) will shift the focus away from technology and rely more on to the collaboration between humans and AI-powered robots. This approach emphasizes a more human-centric perspective, enhanced resilience, optimized workplace processes, and a stronger commitment to sustainability. The humanoid robot market has experienced substantial growth, fueled by technological advancements and the increasing need for automation in industries such as service, customer support, and education. However, challenges like high costs, complex maintenance, and societal concerns about job displacement remain. Despite these issues, the market is expected to continue expanding, supported by innovations that enhance both accessibility and performance. Therefore, this article proposes the design and implementation of low-cost, remotely controlled humanoid robots via a mobile application for home-assistant applications. The humanoid robot boasts an advanced mechanical structure, high-performance actuators, and an array of sensors that empower it to execute a wide range of tasks with human-like dexterity and mobility. Incorporating sophisticated control algorithms and a user-friendly Graphical User Interface (GUI) provides precise and stable robot operation and control. Through an in-house developed code, our research contributes to the growing field of humanoid robotics and underscores the significance of advanced control systems in fully harnessing the capabilities of these human-like machines. The implications of our findings extend to the future development and deployment of humanoid robots across various industries and societal contexts, making this an ideal area for students and researchers to explore innovative solutions.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Industry 5.0</span><span class="tag">Humanoid Robots</span><span class="tag">Human-Centric Automation</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2507.02600" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/ArtGS_img.png" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2507.02600" target="_blank" rel="noopener">ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and Manipulation of Articulated Objects</a>
        </h4>
        <p class="cite-brief">Articulated object manipulation remains a critical challenge in robotics due to the complex kinematic constraints and the limited physical reasoning of existing methods. In this work, we introduce ArtGS, a novel framework that extends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling for articulated object understanding and interaction. ArtGS begins with multi-view RGB-D reconstruction, followed by reasoning with a vision-language model (VLM) to extract semantic and structural information, particularly the articulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS optimizes the parameters of the articulated bones, ensuring physically consistent motion constraints and enhancing the manipulation policy. By leveraging dynamic Gaussian splatting, cross-embodiment adaptability, and closed-loop optimization, ArtGS establishes a new framework for efficient, scalable, and generalizable articulated object modeling and manipulation. Experiments conducted in both simulation and real-world environments demonstrate that ArtGS significantly outperforms previous methods in joint estimation accuracy and manipulation success rates across a variety of articulated objects. </p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">ArtGS</span><span class="tag">Articulated Object Manipulation</span><span class="tag">3D Gaussian Splatting</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2507.14809" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/Light Future_img.png" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2507.14809" target="_blank" rel="noopener">Light Future: Multimodal Action Frame Prediction via InstructPix2Pix</a>
        </h4>
        <p class="cite-brief">Predicting future motion trajectories is a critical capability across domains such as robotics, autonomous systems, and human activity forecasting, enabling safer and more intelligent decision-making. This paper proposes a novel, efficient, and lightweight approach for robot action prediction, offering significantly reduced computational cost and inference latency compared to conventional video prediction models. Importantly, it pioneers the adaptation of the InstructPix2Pix model for forecasting future visual frames in robotic tasks, extending its utility beyond static image editing. We implement a deep learning-based visual prediction framework that forecasts what a robot will observe 100 frames (10 seconds) into the future, given a current image and a textual instruction. We repurpose and fine-tune the InstructPix2Pix model to accept both visual and textual inputs, enabling multimodal future frame prediction. Experiments on the RoboTWin dataset (generated based on real-world scenarios) demonstrate that our method achieves superior SSIM and PSNR compared to state-of-the-art baselines in robot action prediction tasks. Unlike conventional video prediction models that require multiple input frames, heavy computation, and slow inference latency, our approach only needs a single image and a text prompt as input. This lightweight design enables faster inference, reduced GPU demands, and flexible multimodal control, particularly valuable for applications like robotics and sports motion trajectory analytics, where motion trajectory precision is prioritized over visual fidelity.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">InstructPix2Pix</span><span class="tag">Robot Action Prediction</span><span class="tag">Future Frame Forecasting</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.175624610.06665789" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/Recipe for Vision-Language-Action Models in Robotic Manipulation_img.png" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.175624610.06665789" target="_blank" rel="noopener">Recipe for Vision-Language-Action Models in Robotic Manipulation: A Survey</a>
        </h4>
        <p class="cite-brief">This survey provides an in-depth analysis of recent advancements in foundational models that leverage large and diverse datasets to enable flexible and general-purpose manipulation skills. We reviewed the main concepts of Vision-Language-Action (VLA) models with emphasis on their connection to broader trends in robot learning and the architectural strategies that facilitate integration across vision, language, and action. The survey contains current training methodologies, dataset construction techniques, and the role of high-quality demonstrations in achieving reliable performance across diverse tasks. Key trends are identified, including the adoption of open-source tools, scaling of model size and training data, and improved generalization across platforms. In addition, we noted that VLA models demonstrate strong performance in controlled environments; however, significant challenges persist in long-term task planning, failure recovery, and operation in complex real-world scenarios. This survey aims to serve as a comprehensive reference for researchers and outline future research directions that may advance the practical deployment of VLA models in robotic manipulation.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Vision-Language-Action Models</span><span class="tag">Robotic Manipulation</span><span class="tag">Foundational Models Survey</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://www.researchgate.net/profile/Georgios-Drakopoulos/publication/393671925_Linear_Algebraic_Properties_Of_Quantum_Gates_As_A_Starting_Point_For_Their_Digital_Twinning/links/687538eedd6b84447df876f2/Linear-Algebraic-Properties-Of-Quantum-Gates-As-A-Starting-Point-For-Their-Digital-Twinning.pdf" target="_blank" rel="noopener">
        <img src="../img/tab_v1.0_img/Linear Algebraic Properties_img.png" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://www.researchgate.net/profile/Georgios-Drakopoulos/publication/393671925_Linear_Algebraic_Properties_Of_Quantum_Gates_As_A_Starting_Point_For_Their_Digital_Twinning/links/687538eedd6b84447df876f2/Linear-Algebraic-Properties-Of-Quantum-Gates-As-A-Starting-Point-For-Their-Digital-Twinning.pdf" target="_blank" rel="noopener">Linear Algebraic Properties Of Quantum Gates As A Starting Point For Their Digital Twinning</a>
        </h4>
        <p class="cite-brief">Quantum circuits are one of the three universal quantum computing paradigms and by far the most popular and intuitive one, the two other being quantum walks and adiabatic quantum computing. Ad_x0002_ditionally, quantum circuits can verify the validity of computations done in special purpose paradigms like quantum annealing. From an architec_x0002_tural perspective the most important components of quantum circuits are quantum gates and the connectivity between them. Because of the physical properties and the limitations of existing implementations, it is paramount that quantum gate operation be digitally twinned in classical computers for verification as well as study and observation purposes. A starting point is the detailed description of their linear algebraic prop_x0002_erties since each quantum gate corresponds to a unitary operator on a suitably defined Hilbert space. Said properties include the matrix form, the eigenvalues, the determinant, the trace, the effect of basis qubits, and their geometric interpretations, as the latter is fundamental in un_x0002_derstanding quantum computing. Additionally, it is explored how these properties can be directly translated to proper Pythonic code.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Quantum Circuits</span><span class="tag">Quantum Gates</span><span class="tag">Digital Twin Verification</span>
        </div>
      </div>
    </article>
    
  </section>
</body>
</html>
